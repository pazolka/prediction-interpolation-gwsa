{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6881e73d-4755-4ead-b32a-8c0602fb052a",
   "metadata": {},
   "source": [
    "# 12.5 DeepKriging ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ff10aa-8632-4ead-ac94-003df83298e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 19:29:33.534984: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-16 19:29:33.730504: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.metrics import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "import models.stdk as stdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9c201-7fe7-4446-9559-53cfc112fbc6",
   "metadata": {},
   "source": [
    "#### 1. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf4e958-6b49-4a01-9c40-f58a31d54f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './data/Bangladesh/'\n",
    "\n",
    "bgd_shp = gpd.read_file(f'{root_path}/BGD_shp/vb025yv7599.shp')\n",
    "info_file = pd.read_csv(f'{root_path}/target/well_info.csv')\n",
    "\n",
    "lag=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7d8998-17a0-4795-9ad7-4d6a54a297ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_setup = np.load(f'{root_path}/cv_setup.npy', allow_pickle=True).item()\n",
    "train_ids = cv_setup['train_ids']\n",
    "holdout_ids = cv_setup['holdout_ids']\n",
    "tr_idxs = cv_setup['tr_idxs']\n",
    "test_idxs = cv_setup['test_idxs']\n",
    "seed = cv_setup['seed']\n",
    "\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4cdeab-9217-44b4-ad3c-6603c9b45637",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_era_ds = xr.open_mfdataset(f'{root_path}/inputs/PR_AET_M_2002_2017_BGD_025.nc4')\n",
    "full_era_ds['time'] = [x + pd.DateOffset(days=14) for x in full_era_ds['time'].values ]\n",
    "full_precip_da = full_era_ds['pr'][(lag-1):]\n",
    "full_aet_da = full_era_ds['aet'][(lag-1):]\n",
    "\n",
    "full_twsa_da = xr.open_mfdataset(f'{root_path}/inputs/TWSA_M_2002_2017_BGD_025.nc4')['twsa'][(lag-1):]\n",
    "full_twsa_da['time'] = [x + pd.DateOffset(days=14) for x in full_twsa_da['time'].values ]\n",
    "\n",
    "dem_da = xr.open_mfdataset(f'{root_path}/inputs/DEM_BGD_025.nc4')['dem'][0]\n",
    "full_ndvi_da = xr.open_mfdataset(f'{root_path}/inputs/NDVI_M_2002_2017_BGD_025.nc4')['NDVI'][(lag-1):]\n",
    "full_ndvi_da['time'] = [x + pd.DateOffset(days=14) for x in full_ndvi_da['time'].values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a8784a-da9b-40d5-8c86-b6f559e8504f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FA062</th>\n",
       "      <th>RA-63</th>\n",
       "      <th>DI077</th>\n",
       "      <th>BO033</th>\n",
       "      <th>PT005</th>\n",
       "      <th>DI058</th>\n",
       "      <th>RAJ-A-4</th>\n",
       "      <th>FA012</th>\n",
       "      <th>BA003</th>\n",
       "      <th>FA014_1</th>\n",
       "      <th>...</th>\n",
       "      <th>CT011_1</th>\n",
       "      <th>RA-06_1</th>\n",
       "      <th>SY011_1</th>\n",
       "      <th>KH009_1</th>\n",
       "      <th>PA012_1</th>\n",
       "      <th>KT041_1</th>\n",
       "      <th>BO020_1</th>\n",
       "      <th>RA-39_1</th>\n",
       "      <th>MY073</th>\n",
       "      <th>CM016_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-08-15</th>\n",
       "      <td>0.035732</td>\n",
       "      <td>0.266229</td>\n",
       "      <td>0.167378</td>\n",
       "      <td>0.126731</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>0.188272</td>\n",
       "      <td>0.219965</td>\n",
       "      <td>0.181349</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.348828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038336</td>\n",
       "      <td>0.076653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037171</td>\n",
       "      <td>0.221155</td>\n",
       "      <td>0.183615</td>\n",
       "      <td>0.118237</td>\n",
       "      <td>0.248093</td>\n",
       "      <td>0.069225</td>\n",
       "      <td>0.092538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-09-15</th>\n",
       "      <td>0.028685</td>\n",
       "      <td>0.192840</td>\n",
       "      <td>0.133967</td>\n",
       "      <td>0.109969</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.161430</td>\n",
       "      <td>0.236155</td>\n",
       "      <td>0.169446</td>\n",
       "      <td>0.041755</td>\n",
       "      <td>0.305976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037981</td>\n",
       "      <td>0.063972</td>\n",
       "      <td>0.014714</td>\n",
       "      <td>0.039259</td>\n",
       "      <td>0.152881</td>\n",
       "      <td>0.235851</td>\n",
       "      <td>0.088037</td>\n",
       "      <td>0.184756</td>\n",
       "      <td>0.092793</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-10-15</th>\n",
       "      <td>0.012771</td>\n",
       "      <td>0.187793</td>\n",
       "      <td>0.154595</td>\n",
       "      <td>0.102847</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>0.157403</td>\n",
       "      <td>0.211211</td>\n",
       "      <td>0.122933</td>\n",
       "      <td>0.034546</td>\n",
       "      <td>0.209994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>0.043940</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>0.030290</td>\n",
       "      <td>0.136781</td>\n",
       "      <td>0.187911</td>\n",
       "      <td>0.189162</td>\n",
       "      <td>0.144253</td>\n",
       "      <td>0.097922</td>\n",
       "      <td>0.037301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-11-15</th>\n",
       "      <td>-0.001840</td>\n",
       "      <td>0.086861</td>\n",
       "      <td>0.150209</td>\n",
       "      <td>0.067961</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.095833</td>\n",
       "      <td>0.136641</td>\n",
       "      <td>0.080829</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.066394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039353</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>0.010918</td>\n",
       "      <td>0.026380</td>\n",
       "      <td>0.041796</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>0.022146</td>\n",
       "      <td>0.049879</td>\n",
       "      <td>0.092824</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-12-15</th>\n",
       "      <td>-0.012661</td>\n",
       "      <td>0.015540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045235</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.048690</td>\n",
       "      <td>0.073910</td>\n",
       "      <td>0.030656</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>-0.036177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038387</td>\n",
       "      <td>-0.010128</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>-0.018899</td>\n",
       "      <td>0.017024</td>\n",
       "      <td>0.022146</td>\n",
       "      <td>-0.020170</td>\n",
       "      <td>0.082071</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-15</th>\n",
       "      <td>-0.001840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009045</td>\n",
       "      <td>-0.055977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.030009</td>\n",
       "      <td>-0.001467</td>\n",
       "      <td>-0.079112</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.104371</td>\n",
       "      <td>-0.045727</td>\n",
       "      <td>0.018628</td>\n",
       "      <td>-0.065246</td>\n",
       "      <td>-0.082508</td>\n",
       "      <td>-0.103230</td>\n",
       "      <td>-0.068590</td>\n",
       "      <td>-0.103694</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-15</th>\n",
       "      <td>-0.015199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.015468</td>\n",
       "      <td>-0.090689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.085384</td>\n",
       "      <td>-0.012483</td>\n",
       "      <td>-0.129631</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.068350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.007972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.158661</td>\n",
       "      <td>-0.156309</td>\n",
       "      <td>-0.105594</td>\n",
       "      <td>-0.165713</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-15</th>\n",
       "      <td>-0.014364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.080636</td>\n",
       "      <td>-0.103138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.049119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.104341</td>\n",
       "      <td>-0.017224</td>\n",
       "      <td>-0.165893</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026186</td>\n",
       "      <td>-0.115581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.016535</td>\n",
       "      <td>-0.158614</td>\n",
       "      <td>-0.183723</td>\n",
       "      <td>-0.165461</td>\n",
       "      <td>-0.208286</td>\n",
       "      <td>-0.188839</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-15</th>\n",
       "      <td>-0.005597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.159589</td>\n",
       "      <td>-0.101836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.080156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.140934</td>\n",
       "      <td>-0.011731</td>\n",
       "      <td>-0.202156</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.092976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.042324</td>\n",
       "      <td>-0.131500</td>\n",
       "      <td>-0.201267</td>\n",
       "      <td>-0.124279</td>\n",
       "      <td>-0.189354</td>\n",
       "      <td>-0.126031</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-15</th>\n",
       "      <td>0.003169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.130314</td>\n",
       "      <td>-0.082728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.058681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.082562</td>\n",
       "      <td>0.012378</td>\n",
       "      <td>-0.133319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>-0.056441</td>\n",
       "      <td>-0.012822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.094752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.072115</td>\n",
       "      <td>-0.120682</td>\n",
       "      <td>-0.056180</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 1225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               FA062     RA-63     DI077     BO033     PT005     DI058  \\\n",
       "Date                                                                     \n",
       "2002-08-15  0.035732  0.266229  0.167378  0.126731  0.004849  0.188272   \n",
       "2002-09-15  0.028685  0.192840  0.133967  0.109969  0.004391  0.161430   \n",
       "2002-10-15  0.012771  0.187793  0.154595  0.102847  0.004849  0.157403   \n",
       "2002-11-15 -0.001840  0.086861  0.150209  0.067961  0.003961  0.095833   \n",
       "2002-12-15 -0.012661  0.015540       NaN  0.045235  0.002334  0.048690   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-01-15 -0.001840       NaN  0.009045 -0.055977       NaN       NaN   \n",
       "2017-02-15 -0.015199       NaN -0.015468 -0.090689       NaN       NaN   \n",
       "2017-03-15 -0.014364       NaN -0.080636 -0.103138       NaN -0.049119   \n",
       "2017-04-15 -0.005597       NaN -0.159589 -0.101836       NaN -0.080156   \n",
       "2017-05-15  0.003169       NaN -0.130314 -0.082728       NaN -0.058681   \n",
       "\n",
       "             RAJ-A-4     FA012     BA003   FA014_1  ...   CT011_1   RA-06_1  \\\n",
       "Date                                                ...                       \n",
       "2002-08-15  0.219965  0.181349  0.043500  0.348828  ...  0.038336  0.076653   \n",
       "2002-09-15  0.236155  0.169446  0.041755  0.305976  ...  0.037981  0.063972   \n",
       "2002-10-15  0.211211  0.122933  0.034546  0.209994  ...  0.036050  0.043940   \n",
       "2002-11-15  0.136641  0.080829  0.029730  0.066394  ...  0.039353  0.006265   \n",
       "2002-12-15  0.073910  0.030656  0.015509 -0.036177  ...  0.038387 -0.010128   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2017-01-15       NaN -0.030009 -0.001467 -0.079112  ...       NaN -0.104371   \n",
       "2017-02-15       NaN -0.085384 -0.012483 -0.129631  ...       NaN -0.068350   \n",
       "2017-03-15       NaN -0.104341 -0.017224 -0.165893  ... -0.026186 -0.115581   \n",
       "2017-04-15       NaN -0.140934 -0.011731 -0.202156  ...       NaN -0.092976   \n",
       "2017-05-15       NaN -0.082562  0.012378 -0.133319  ...  0.032900 -0.056441   \n",
       "\n",
       "             SY011_1   KH009_1   PA012_1   KT041_1   BO020_1   RA-39_1  \\\n",
       "Date                                                                     \n",
       "2002-08-15       NaN  0.037171  0.221155  0.183615  0.118237  0.248093   \n",
       "2002-09-15  0.014714  0.039259  0.152881  0.235851  0.088037  0.184756   \n",
       "2002-10-15  0.013045  0.030290  0.136781  0.187911  0.189162  0.144253   \n",
       "2002-11-15  0.010918  0.026380  0.041796  0.096614  0.022146  0.049879   \n",
       "2002-12-15  0.010089  0.019737 -0.018899  0.017024  0.022146 -0.020170   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-01-15 -0.045727  0.018628 -0.065246 -0.082508 -0.103230 -0.068590   \n",
       "2017-02-15       NaN -0.007972       NaN -0.158661 -0.156309 -0.105594   \n",
       "2017-03-15       NaN -0.016535 -0.158614 -0.183723 -0.165461 -0.208286   \n",
       "2017-04-15       NaN -0.042324 -0.131500 -0.201267 -0.124279 -0.189354   \n",
       "2017-05-15 -0.012822       NaN -0.094752       NaN -0.072115 -0.120682   \n",
       "\n",
       "               MY073   CM016_1  \n",
       "Date                            \n",
       "2002-08-15  0.069225  0.092538  \n",
       "2002-09-15  0.092793       NaN  \n",
       "2002-10-15  0.097922  0.037301  \n",
       "2002-11-15  0.092824       NaN  \n",
       "2002-12-15  0.082071       NaN  \n",
       "...              ...       ...  \n",
       "2017-01-15 -0.103694       NaN  \n",
       "2017-02-15 -0.165713       NaN  \n",
       "2017-03-15 -0.188839       NaN  \n",
       "2017-04-15 -0.126031       NaN  \n",
       "2017-05-15 -0.056180       NaN  \n",
       "\n",
       "[178 rows x 1225 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_ts = pd.read_csv(f\"{root_path}/target/filtered_gws_ts_data_1961_2019.csv\", parse_dates=['Date'], index_col='Date')\n",
    "miss_ts = miss_ts['2002-04-01':'2017-05-31'].dropna(axis=1, how='all')[(lag-1):]\n",
    "miss_ts_gwsa = miss_ts - miss_ts['2004-01-01':'2009-12-31'].mean()\n",
    "miss_ts_gwsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da12db1-da8e-4386-866c-ca5e666b43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### prepare training set\n",
    "coords_train = np.array([ info_file[info_file['SegmentID'] == well][['Latitude', 'Longitude']].values[0] for well in train_ids ])\n",
    "df_train = miss_ts_gwsa[train_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f6ef5a-14df-4032-ae93-aee09ef5c121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"STDK_Interp_ScalarCov\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " coords_xy (InputLayer)      [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " time_scalar (InputLayer)    [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " phi_space (RBFEmbed)        (None, 64)                   129       ['coords_xy[0][0]']           \n",
      "                                                                                                  \n",
      " phi_time (RBFEmbed)         (None, 16)                   17        ['time_scalar[0][0]']         \n",
      "                                                                                                  \n",
      " cov_scalar_vec (InputLayer  [(None, 5)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 85)                   0         ['phi_space[0][0]',           \n",
      "                                                                     'phi_time[0][0]',            \n",
      "                                                                     'cov_scalar_vec[0][0]']      \n",
      "                                                                                                  \n",
      " dnn_0 (Dense)               (None, 256)                  22016     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 256)                  0         ['dnn_0[0][0]']               \n",
      "                                                                                                  \n",
      " dnn_1 (Dense)               (None, 128)                  32896     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 128)                  0         ['dnn_1[0][0]']               \n",
      "                                                                                                  \n",
      " dnn_2 (Dense)               (None, 64)                   8256      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 64)                   0         ['dnn_2[0][0]']               \n",
      "                                                                                                  \n",
      " q_out (Dense)               (None, 3)                    195       ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 63509 (248.08 KB)\n",
      "Trainable params: 63363 (247.51 KB)\n",
      "Non-trainable params: 146 (584.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "stdk.build_interpolator_scalar_cov(\n",
    "    n_space_basis=64, n_time_basis=16,\n",
    "    ls_space=0.15, ls_time=0.10,\n",
    "    hidden=(256,128,64), dropout=0.10,\n",
    "    quantiles=(0.1,0.5,0.9), n_scalar_cov=5\n",
    ").summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d668fa70-aa36-47c0-a52b-44b6bae875db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 12, 24, 24, 5)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 12, 24, 24, 32)    42752     \n",
      "                                                                 \n",
      " conv_lstm2d_3 (ConvLSTM2D)  (None, 24, 24, 32)        73856     \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116707 (455.89 KB)\n",
      "Trainable params: 116707 (455.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "stdk.build_grid_forecaster((12, 24, 24, 5), [0.1, 0.5, 0.9], ~dem_da.isnull().values, hidden=(32, 32)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9388ef9-bc33-452b-be69-d20510a73812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 28ms/step - loss: 0.0440 - val_loss: 0.0274\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0274 - val_loss: 0.0205\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0225 - val_loss: 0.0172\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0203 - val_loss: 0.0157\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0189 - val_loss: 0.0149\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0181 - val_loss: 0.0143\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0175 - val_loss: 0.0140\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0172 - val_loss: 0.0138\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0170 - val_loss: 0.0137\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0165 - val_loss: 0.0134\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0164 - val_loss: 0.0133\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0162 - val_loss: 0.0131\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0160 - val_loss: 0.0132\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0159 - val_loss: 0.0130\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0157 - val_loss: 0.0130\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0157 - val_loss: 0.0129\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0157 - val_loss: 0.0129\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0155 - val_loss: 0.0128\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0154 - val_loss: 0.0128\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0154 - val_loss: 0.0128\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0153 - val_loss: 0.0128\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0152 - val_loss: 0.0127\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0152 - val_loss: 0.0126\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.0126\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.0126\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.0127\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0150 - val_loss: 0.0126\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0150 - val_loss: 0.0125\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0149 - val_loss: 0.0125\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0150 - val_loss: 0.0124\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0149 - val_loss: 0.0123\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0124\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 0.0124\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0147 - val_loss: 0.0123\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 0.0124\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0147 - val_loss: 0.0124\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0147 - val_loss: 0.0125\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 0.0123\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 0.0124\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 0.0124\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 0.0124\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0145 - val_loss: 0.0123\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0145 - val_loss: 0.0122\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0144 - val_loss: 0.0124\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0145 - val_loss: 0.0121\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.0122\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0123\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.0123\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0122\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0121\n",
      "Anomaly training vmin/vmax: -0.19175749 0.3227415\n",
      "Exog fitted mins: [-3.1981788197299466e-05, -0.0022247382439672947, 0.012256748974323273, 0.018415797501802444, -0.000681682548020035]\n",
      "Exog fitted maxs: [2.1423933506011963, 0.9917258620262146, 0.9078999161720276, 1.0443754196166992, 3.554227113723755]\n",
      "Target land min/max (expect ~0..1): 0.0 1.0\n",
      "Target ocean unique (expect [-1]): [-1.]\n",
      "Exog land min/max (expect ~0..1): 0.0 1.0\n",
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 12, 24, 24, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 12, 24, 24, 32)    43904     \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 24, 24, 32)        73856     \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117859 (460.39 KB)\n",
      "Trainable params: 117859 (460.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.3198 - val_loss: 0.5727\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.5882 - val_loss: 0.4941\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.3379 - val_loss: 0.5160\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.3305 - val_loss: 0.3865\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.2729 - val_loss: 0.5601\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.3360 - val_loss: 0.4709\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.2473 - val_loss: 0.4802\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.2773 - val_loss: 0.4355\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.2558 - val_loss: 0.3494\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.2080 - val_loss: 0.2926\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.1902 - val_loss: 0.3042\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.2023 - val_loss: 0.3329\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.1837 - val_loss: 0.3536\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.1885 - val_loss: 0.3522\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.1928 - val_loss: 0.3372\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 1s 19ms/step - loss: 0.0415 - val_loss: 0.0265\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0234 - val_loss: 0.0213\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0198 - val_loss: 0.0190\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0181 - val_loss: 0.0181\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0172 - val_loss: 0.0176\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0165 - val_loss: 0.0173\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0162 - val_loss: 0.0169\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0158 - val_loss: 0.0169\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.0156 - val_loss: 0.0169\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0153 - val_loss: 0.0167\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0153 - val_loss: 0.0166\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0165\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0149 - val_loss: 0.0165\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0148 - val_loss: 0.0164\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 0.0164\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0145 - val_loss: 0.0162\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0146 - val_loss: 0.0163\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0145 - val_loss: 0.0162\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0162\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0143 - val_loss: 0.0161\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0142 - val_loss: 0.0161\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0142 - val_loss: 0.0161\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0141 - val_loss: 0.0160\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0140 - val_loss: 0.0160\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0140 - val_loss: 0.0160\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0140 - val_loss: 0.0160\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0160\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0161\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0159\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0159\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0137 - val_loss: 0.0158\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0158\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0136 - val_loss: 0.0159\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0136 - val_loss: 0.0158\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0136 - val_loss: 0.0158\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0158\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0136 - val_loss: 0.0158\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0158\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0158\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0135 - val_loss: 0.0159\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0158\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0133 - val_loss: 0.0158\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0132 - val_loss: 0.0158\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0158\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0158\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0132 - val_loss: 0.0157\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0132 - val_loss: 0.0158\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.0132 - val_loss: 0.0158\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0159\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0158\n",
      "Anomaly training vmin/vmax: -0.25527105 0.4484924\n",
      "Exog fitted mins: [-0.00017035573546309024, -0.013653010129928589, 0.0, 0.0, -0.000681682548020035]\n",
      "Exog fitted maxs: [1.7632609605789185, 1.0255014896392822, 1.0434422492980957, 1.0502550601959229, 3.554227113723755]\n",
      "Target land min/max (expect ~0..1): 0.0 1.0\n",
      "Target ocean unique (expect [-1]): [-1.]\n",
      "Exog land min/max (expect ~0..1): 0.0 1.0\n",
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 12, 24, 24, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 12, 24, 24, 32)    43904     \n",
      "                                                                 \n",
      " conv_lstm2d_3 (ConvLSTM2D)  (None, 24, 24, 32)        73856     \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117859 (460.39 KB)\n",
      "Trainable params: 117859 (460.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 4s 396ms/step - loss: 0.6489 - val_loss: 0.4676\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 1s 184ms/step - loss: 0.4012 - val_loss: 0.3408\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 1s 182ms/step - loss: 0.2950 - val_loss: 0.3061\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.2542 - val_loss: 0.2673\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 1s 184ms/step - loss: 0.2370 - val_loss: 0.2151\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 0.2056 - val_loss: 0.2215\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.1852 - val_loss: 0.1835\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.1659 - val_loss: 0.1857\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.1519 - val_loss: 0.1639\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.1520 - val_loss: 0.1593\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 1s 197ms/step - loss: 0.1444 - val_loss: 0.1485\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 1s 206ms/step - loss: 0.1511 - val_loss: 0.1944\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 1s 191ms/step - loss: 0.1564 - val_loss: 0.1577\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 1s 193ms/step - loss: 0.1441 - val_loss: 0.1557\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.1362 - val_loss: 0.1446\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.1287 - val_loss: 0.1467\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 1s 193ms/step - loss: 0.1243 - val_loss: 0.1336\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 1s 193ms/step - loss: 0.1214 - val_loss: 0.1330\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 1s 205ms/step - loss: 0.1182 - val_loss: 0.1298\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 1s 202ms/step - loss: 0.1128 - val_loss: 0.1318\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 1s 196ms/step - loss: 0.1128 - val_loss: 0.1262\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 1s 194ms/step - loss: 0.1097 - val_loss: 0.1501\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 1s 194ms/step - loss: 0.1137 - val_loss: 0.1313\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 1s 195ms/step - loss: 0.1221 - val_loss: 0.1195\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 1s 197ms/step - loss: 0.1130 - val_loss: 0.1290\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 1s 199ms/step - loss: 0.1082 - val_loss: 0.1186\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 1s 199ms/step - loss: 0.1076 - val_loss: 0.1339\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 1s 203ms/step - loss: 0.0948 - val_loss: 0.1261\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 1s 201ms/step - loss: 0.1060 - val_loss: 0.1317\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 1s 197ms/step - loss: 0.0981 - val_loss: 0.1196\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 1s 199ms/step - loss: 0.0972 - val_loss: 0.1047\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 1s 210ms/step - loss: 0.0895 - val_loss: 0.1055\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 1s 202ms/step - loss: 0.0835 - val_loss: 0.1007\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 1s 199ms/step - loss: 0.0819 - val_loss: 0.1009\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 1s 201ms/step - loss: 0.0758 - val_loss: 0.0995\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 1s 199ms/step - loss: 0.0766 - val_loss: 0.1011\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 1s 208ms/step - loss: 0.0758 - val_loss: 0.0900\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 1s 203ms/step - loss: 0.0706 - val_loss: 0.0897\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 1s 203ms/step - loss: 0.0693 - val_loss: 0.0891\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 1s 202ms/step - loss: 0.0709 - val_loss: 0.0979\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 1s 202ms/step - loss: 0.0710 - val_loss: 0.0983\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 1s 202ms/step - loss: 0.0726 - val_loss: 0.0849\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 1s 205ms/step - loss: 0.0705 - val_loss: 0.0932\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 1s 203ms/step - loss: 0.0673 - val_loss: 0.0926\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 1s 201ms/step - loss: 0.0653 - val_loss: 0.0851\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 1s 201ms/step - loss: 0.0630 - val_loss: 0.0859\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 1s 199ms/step - loss: 0.0628 - val_loss: 0.0866\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 22ms/step - loss: 0.0297 - val_loss: 0.0237\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0222 - val_loss: 0.0190\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0190 - val_loss: 0.0162\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0173 - val_loss: 0.0153\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0164 - val_loss: 0.0149\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0158 - val_loss: 0.0146\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0154 - val_loss: 0.0143\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0151 - val_loss: 0.0141\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0150 - val_loss: 0.0143\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0148 - val_loss: 0.0141\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0146 - val_loss: 0.0139\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.0138\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0137\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0136\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0142 - val_loss: 0.0137\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0141 - val_loss: 0.0136\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0140 - val_loss: 0.0137\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0140 - val_loss: 0.0135\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0132\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0139\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0131\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0134\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0135\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0133\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0132\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0131\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0132\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0132\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0133\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0131\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0132 - val_loss: 0.0132\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0133\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0131\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0131\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0133\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0129\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0138\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0130 - val_loss: 0.0131\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 0.0131\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 0.0131\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0130 - val_loss: 0.0132\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 0.0130\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0127 - val_loss: 0.0133\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0127 - val_loss: 0.0133\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0128 - val_loss: 0.0132\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0127 - val_loss: 0.0132\n",
      "Anomaly training vmin/vmax: -0.47445038 0.60388815\n",
      "Exog fitted mins: [-0.00020698523439932615, 0.0, 0.0527864433825016, 0.0, -0.000681682548020035]\n",
      "Exog fitted maxs: [2.1423933506011963, 1.023913860321045, 1.0411490201950073, 1.0502550601959229, 3.554227113723755]\n",
      "Target land min/max (expect ~0..1): 0.0 1.0\n",
      "Target ocean unique (expect [-1]): [-1.]\n",
      "Exog land min/max (expect ~0..1): 0.0 1.0\n",
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 12, 24, 24, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_4 (ConvLSTM2D)  (None, 12, 24, 24, 32)    43904     \n",
      "                                                                 \n",
      " conv_lstm2d_5 (ConvLSTM2D)  (None, 24, 24, 32)        73856     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117859 (460.39 KB)\n",
      "Trainable params: 117859 (460.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 4s 283ms/step - loss: 0.6654 - val_loss: 0.3624\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 0.2680 - val_loss: 0.2331\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.1891 - val_loss: 0.1788\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.1482 - val_loss: 0.1475\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.1404 - val_loss: 0.1195\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.1236 - val_loss: 0.1118\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 1s 190ms/step - loss: 0.1202 - val_loss: 0.1116\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 1s 190ms/step - loss: 0.1143 - val_loss: 0.1065\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 0.1060 - val_loss: 0.1011\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 1s 191ms/step - loss: 0.1021 - val_loss: 0.0959\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.0982 - val_loss: 0.0959\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.0951 - val_loss: 0.0987\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 0.0947 - val_loss: 0.0886\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.0911 - val_loss: 0.0875\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 1s 194ms/step - loss: 0.0888 - val_loss: 0.0838\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.0844 - val_loss: 0.0855\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.0834 - val_loss: 0.0825\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.0849 - val_loss: 0.0844\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.0846 - val_loss: 0.0823\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.0819 - val_loss: 0.0817\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.0844 - val_loss: 0.0825\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 0.0841 - val_loss: 0.0837\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 1s 198ms/step - loss: 0.0821 - val_loss: 0.0852\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.0806 - val_loss: 0.0748\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.0741 - val_loss: 0.0772\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.0762 - val_loss: 0.0758\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 1s 197ms/step - loss: 0.0827 - val_loss: 0.0939\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 1s 198ms/step - loss: 0.0830 - val_loss: 0.0753\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.0786 - val_loss: 0.0851\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 19ms/step - loss: 0.0335 - val_loss: 0.0187\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0224 - val_loss: 0.0148\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0193 - val_loss: 0.0133\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0179 - val_loss: 0.0128\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0170 - val_loss: 0.0124\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0164 - val_loss: 0.0122\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0161 - val_loss: 0.0119\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.0157 - val_loss: 0.0117\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0155 - val_loss: 0.0117\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0152 - val_loss: 0.0117\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0151 - val_loss: 0.0115\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0149 - val_loss: 0.0114\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 0.0113\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 0.0113\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0145 - val_loss: 0.0114\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0145 - val_loss: 0.0114\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.0112\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0142 - val_loss: 0.0112\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0142 - val_loss: 0.0112\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0141 - val_loss: 0.0112\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0113\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0111\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0112\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0111\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0112\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0112\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0114\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0112\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0111\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0112\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0112\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0111\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0132 - val_loss: 0.0110\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0132 - val_loss: 0.0111\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0112\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0132 - val_loss: 0.0110\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0110\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0111\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0130 - val_loss: 0.0110\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0130 - val_loss: 0.0111\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 0.0111\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.0129 - val_loss: 0.0110\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 0.0111\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 0.0111\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0128 - val_loss: 0.0110\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0128 - val_loss: 0.0110\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 11ms/step - loss: 0.0128 - val_loss: 0.0110\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.0128 - val_loss: 0.0111\n",
      "Anomaly training vmin/vmax: -0.3419686 0.50903237\n",
      "Exog fitted mins: [-0.00017035573546309024, 0.0, 0.0, 0.0, -0.000681682548020035]\n",
      "Exog fitted maxs: [1.7632609605789185, 1.023913860321045, 1.0411490201950073, 1.069959282875061, 3.554227113723755]\n",
      "Target land min/max (expect ~0..1): 0.0 1.0\n",
      "Target ocean unique (expect [-1]): [-1.]\n",
      "Exog land min/max (expect ~0..1): 0.0 1.0\n",
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 12, 24, 24, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_6 (ConvLSTM2D)  (None, 12, 24, 24, 32)    43904     \n",
      "                                                                 \n",
      " conv_lstm2d_7 (ConvLSTM2D)  (None, 24, 24, 32)        73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117859 (460.39 KB)\n",
      "Trainable params: 117859 (460.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 5s 252ms/step - loss: 0.7170 - val_loss: 0.3478\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 1s 186ms/step - loss: 0.3047 - val_loss: 0.2221\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 1s 188ms/step - loss: 0.1978 - val_loss: 0.2061\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 1s 187ms/step - loss: 0.1715 - val_loss: 0.1750\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 1s 192ms/step - loss: 0.1458 - val_loss: 0.1618\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 1s 193ms/step - loss: 0.1299 - val_loss: 0.1555\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 1s 193ms/step - loss: 0.1243 - val_loss: 0.1378\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 1s 205ms/step - loss: 0.1104 - val_loss: 0.1284\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 1s 197ms/step - loss: 0.1029 - val_loss: 0.1296\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 1s 199ms/step - loss: 0.1007 - val_loss: 0.1243\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 1s 197ms/step - loss: 0.0980 - val_loss: 0.1205\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 1s 198ms/step - loss: 0.0937 - val_loss: 0.1159\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.0915 - val_loss: 0.1127\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 1s 208ms/step - loss: 0.0892 - val_loss: 0.1139\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 1s 209ms/step - loss: 0.0918 - val_loss: 0.1072\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0856 - val_loss: 0.1096\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0858 - val_loss: 0.1089\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0831 - val_loss: 0.1101\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0835 - val_loss: 0.1027\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0828 - val_loss: 0.1071\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 1s 205ms/step - loss: 0.0854 - val_loss: 0.0968\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.0765 - val_loss: 0.0916\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 1s 206ms/step - loss: 0.0721 - val_loss: 0.0890\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 1s 211ms/step - loss: 0.0692 - val_loss: 0.0842\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 1s 207ms/step - loss: 0.0684 - val_loss: 0.0849\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 1s 206ms/step - loss: 0.0708 - val_loss: 0.0889\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 1s 214ms/step - loss: 0.0671 - val_loss: 0.0807\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.0662 - val_loss: 0.0883\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 1s 212ms/step - loss: 0.0651 - val_loss: 0.0774\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.0600 - val_loss: 0.0751\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.0613 - val_loss: 0.0756\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.0604 - val_loss: 0.0729\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.0614 - val_loss: 0.0742\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.0646 - val_loss: 0.0943\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 1s 209ms/step - loss: 0.0739 - val_loss: 0.0837\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 1s 214ms/step - loss: 0.0680 - val_loss: 0.0820\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 1s 212ms/step - loss: 0.0635 - val_loss: 0.0714\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.0601 - val_loss: 0.0764\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.0582 - val_loss: 0.0748\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 1s 210ms/step - loss: 0.0614 - val_loss: 0.0687\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 1s 209ms/step - loss: 0.0565 - val_loss: 0.0680\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 1s 211ms/step - loss: 0.0553 - val_loss: 0.0761\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 2s 223ms/step - loss: 0.0614 - val_loss: 0.0726\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 2s 214ms/step - loss: 0.0559 - val_loss: 0.0713\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 1s 211ms/step - loss: 0.0574 - val_loss: 0.0696\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 1s 211ms/step - loss: 0.0598 - val_loss: 0.0972\n",
      "Epoch 1/50\n",
      "26/26 [==============================] - 1s 17ms/step - loss: 0.0324 - val_loss: 0.0219\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0223 - val_loss: 0.0175\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0192 - val_loss: 0.0159\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0178 - val_loss: 0.0151\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0169 - val_loss: 0.0146\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0164 - val_loss: 0.0145\n",
      "Epoch 7/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0160 - val_loss: 0.0142\n",
      "Epoch 8/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0157 - val_loss: 0.0142\n",
      "Epoch 9/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0155 - val_loss: 0.0139\n",
      "Epoch 10/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0153 - val_loss: 0.0138\n",
      "Epoch 11/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.0137\n",
      "Epoch 12/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0150 - val_loss: 0.0135\n",
      "Epoch 13/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0149 - val_loss: 0.0136\n",
      "Epoch 14/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 0.0135\n",
      "Epoch 15/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0147 - val_loss: 0.0134\n",
      "Epoch 16/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0145 - val_loss: 0.0133\n",
      "Epoch 17/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.0132\n",
      "Epoch 18/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0133\n",
      "Epoch 19/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0132\n",
      "Epoch 20/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0142 - val_loss: 0.0131\n",
      "Epoch 21/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0141 - val_loss: 0.0130\n",
      "Epoch 22/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0141 - val_loss: 0.0131\n",
      "Epoch 23/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0140 - val_loss: 0.0130\n",
      "Epoch 24/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0129\n",
      "Epoch 25/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0130\n",
      "Epoch 26/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0130\n",
      "Epoch 27/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0137 - val_loss: 0.0129\n",
      "Epoch 28/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0137 - val_loss: 0.0128\n",
      "Epoch 29/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0129\n",
      "Epoch 30/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0128\n",
      "Epoch 31/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0128\n",
      "Epoch 32/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0135 - val_loss: 0.0128\n",
      "Epoch 33/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0127\n",
      "Epoch 34/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0128\n",
      "Epoch 35/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0127\n",
      "Epoch 36/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0128\n",
      "Epoch 37/50\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.0133 - val_loss: 0.0127\n",
      "Epoch 38/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0127\n",
      "Epoch 39/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0133 - val_loss: 0.0128\n",
      "Epoch 40/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0132 - val_loss: 0.0127\n",
      "Epoch 41/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0133 - val_loss: 0.0128\n",
      "Epoch 42/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0131 - val_loss: 0.0126\n",
      "Epoch 43/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0128\n",
      "Epoch 44/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0131 - val_loss: 0.0125\n",
      "Epoch 45/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0130 - val_loss: 0.0126\n",
      "Epoch 46/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0130 - val_loss: 0.0126\n",
      "Epoch 47/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0130 - val_loss: 0.0126\n",
      "Epoch 48/50\n",
      "26/26 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 0.0124\n",
      "Epoch 49/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0129 - val_loss: 0.0125\n",
      "Epoch 50/50\n",
      "26/26 [==============================] - 0s 10ms/step - loss: 0.0129 - val_loss: 0.0125\n",
      "Anomaly training vmin/vmax: -0.36492932 0.60942346\n",
      "Exog fitted mins: [-0.0003150640695821494, -0.011121206916868687, 0.0, 0.0, -0.000681682548020035]\n",
      "Exog fitted maxs: [1.7632609605789185, 1.023913860321045, 1.0411490201950073, 1.0502550601959229, 3.554227113723755]\n",
      "Target land min/max (expect ~0..1): 0.0 1.0\n",
      "Target ocean unique (expect [-1]): [-1.]\n",
      "Exog land min/max (expect ~0..1): 0.0 1.0\n",
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 12, 24, 24, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_8 (ConvLSTM2D)  (None, 12, 24, 24, 32)    43904     \n",
      "                                                                 \n",
      " conv_lstm2d_9 (ConvLSTM2D)  (None, 24, 24, 32)        73856     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117859 (460.39 KB)\n",
      "Trainable params: 117859 (460.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 5s 236ms/step - loss: 0.5153 - val_loss: 0.2441\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 2s 192ms/step - loss: 0.2213 - val_loss: 0.1599\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1461 - val_loss: 0.1235\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 0.1262 - val_loss: 0.1004\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 0.1125 - val_loss: 0.0895\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 0.1079 - val_loss: 0.0938\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 2s 202ms/step - loss: 0.0984 - val_loss: 0.0846\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 2s 208ms/step - loss: 0.0940 - val_loss: 0.0808\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 0.0898 - val_loss: 0.0839\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 0.0861 - val_loss: 0.0807\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 2s 207ms/step - loss: 0.0832 - val_loss: 0.0918\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 0.0816 - val_loss: 0.0775\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 2s 208ms/step - loss: 0.0780 - val_loss: 0.0734\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 0.0757 - val_loss: 0.0896\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 2s 212ms/step - loss: 0.0778 - val_loss: 0.0722\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 2s 210ms/step - loss: 0.0741 - val_loss: 0.0658\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 2s 216ms/step - loss: 0.0765 - val_loss: 0.0732\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 0.0745 - val_loss: 0.0829\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 2s 210ms/step - loss: 0.0710 - val_loss: 0.0711\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 0.0692 - val_loss: 0.0770\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 0.0688 - val_loss: 0.0683\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0345 - val_loss: 0.0230\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0219 - val_loss: 0.0181\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0188 - val_loss: 0.0169\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0177 - val_loss: 0.0162\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0170 - val_loss: 0.0159\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0166 - val_loss: 0.0155\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0163 - val_loss: 0.0155\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0160 - val_loss: 0.0153\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0157 - val_loss: 0.0153\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0156 - val_loss: 0.0152\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0154 - val_loss: 0.0150\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0153 - val_loss: 0.0149\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.0149\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 0s 12ms/step - loss: 0.0150 - val_loss: 0.0147\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0149 - val_loss: 0.0148\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 0.0147\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 0.0146\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0146 - val_loss: 0.0145\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 0.0144\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0145 - val_loss: 0.0144\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0144\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0142 - val_loss: 0.0142\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0141 - val_loss: 0.0144\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0145\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0138\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0138\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0139\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0140\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0137 - val_loss: 0.0138\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0138\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0137\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0135 - val_loss: 0.0141\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0139\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0139\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0137\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0139\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0139\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0138\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 0s 10ms/step - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0138\n",
      "Anomaly training vmin/vmax: -0.35449547 0.4833998\n",
      "Exog fitted mins: [-0.00031506409868597984, -0.010901917703449726, 0.008611571975052357, 0.0, -0.000681682548020035]\n",
      "Exog fitted maxs: [1.7632609605789185, 1.003724217414856, 1.0391277074813843, 1.0502550601959229, 3.554227113723755]\n",
      "Target land min/max (expect ~0..1): 0.0 1.0\n",
      "Target ocean unique (expect [-1]): [-1.]\n",
      "Exog land min/max (expect ~0..1): 0.0 1.0\n",
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 12, 24, 24, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_10 (ConvLSTM2D  (None, 12, 24, 24, 32)    43904     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_11 (ConvLSTM2D  (None, 24, 24, 32)        73856     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117859 (460.39 KB)\n",
      "Trainable params: 117859 (460.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "11/11 [==============================] - 5s 230ms/step - loss: 0.4896 - val_loss: 0.3683\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.2178 - val_loss: 0.2049\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 0.1596 - val_loss: 0.1667\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 0.1326 - val_loss: 0.1406\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 0.1239 - val_loss: 0.1184\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 0.1115 - val_loss: 0.1240\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 0.1077 - val_loss: 0.1548\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 0.1082 - val_loss: 0.1201\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 0.1061 - val_loss: 0.1263\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 0.0975 - val_loss: 0.1161\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 0.0938 - val_loss: 0.1104\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 0.0951 - val_loss: 0.1141\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 2s 214ms/step - loss: 0.0918 - val_loss: 0.1113\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 0.0890 - val_loss: 0.1062\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 0.0849 - val_loss: 0.1163\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 2s 217ms/step - loss: 0.0833 - val_loss: 0.1030\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 0.0834 - val_loss: 0.1041\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 0.0765 - val_loss: 0.0941\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 0.0740 - val_loss: 0.1011\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 2s 226ms/step - loss: 0.0708 - val_loss: 0.0877\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 0.0718 - val_loss: 0.1010\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 0.0689 - val_loss: 0.0952\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 0.0702 - val_loss: 0.0991\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 0.0705 - val_loss: 0.0899\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 2s 214ms/step - loss: 0.0685 - val_loss: 0.0814\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 0.0711 - val_loss: 0.1014\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 3s 229ms/step - loss: 0.0728 - val_loss: 0.0947\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 0.0706 - val_loss: 0.0859\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 0.0671 - val_loss: 0.0842\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 0.0675 - val_loss: 0.0774\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 0.0682 - val_loss: 0.1156\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 0.0670 - val_loss: 0.0816\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 0.0590 - val_loss: 0.0828\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 2s 225ms/step - loss: 0.0610 - val_loss: 0.0729\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 0.0597 - val_loss: 0.0797\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 0.0588 - val_loss: 0.0697\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 0.0585 - val_loss: 0.0963\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 0.0649 - val_loss: 0.0869\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 0.0597 - val_loss: 0.0795\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 3s 230ms/step - loss: 0.0593 - val_loss: 0.0999\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 2s 212ms/step - loss: 0.0643 - val_loss: 0.0922\n",
      "Epoch 1/50\n",
      "35/35 [==============================] - 1s 15ms/step - loss: 0.0291 - val_loss: 0.0195\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0209 - val_loss: 0.0167\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0186 - val_loss: 0.0158\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0175 - val_loss: 0.0155\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0169 - val_loss: 0.0152\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0165 - val_loss: 0.0150\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0162 - val_loss: 0.0148\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0160 - val_loss: 0.0148\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0157 - val_loss: 0.0146\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0156 - val_loss: 0.0146\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0155 - val_loss: 0.0144\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0153 - val_loss: 0.0143\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0152 - val_loss: 0.0143\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0150 - val_loss: 0.0142\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0142\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0141\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 0.0141\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 0.0139\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 0.0139\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0145 - val_loss: 0.0139\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0145 - val_loss: 0.0139\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.0139\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0138\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0138\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0142 - val_loss: 0.0138\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0141 - val_loss: 0.0138\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0141 - val_loss: 0.0136\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0141 - val_loss: 0.0136\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0140 - val_loss: 0.0135\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0140 - val_loss: 0.0136\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0136\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0136\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0135\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0134\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0135\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0136\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0133\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0135\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0135\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0135\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0134\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0133\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0134\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0133\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0132\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0133\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0135\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0134\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0133\n",
      "Anomaly training vmin/vmax: -0.64695466 0.7236565\n",
      "Exog fitted mins: [-0.00022176191851031035, 0.017536403611302376, 0.06616800278425217, 0.0, -0.000681682548020035]\n",
      "Exog fitted maxs: [1.404210090637207, 1.0036194324493408, 1.0365387201309204, 1.0502550601959229, 3.554227113723755]\n",
      "Target land min/max (expect ~0..1): 0.0 1.0\n",
      "Target ocean unique (expect [-1]): [-1.]\n",
      "Exog land min/max (expect ~0..1): 0.0 1.0\n",
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 12, 24, 24, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_12 (ConvLSTM2D  (None, 12, 24, 24, 32)    43904     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_13 (ConvLSTM2D  (None, 24, 24, 32)        73856     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117859 (460.39 KB)\n",
      "Trainable params: 117859 (460.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 6s 221ms/step - loss: 0.4880 - val_loss: 0.2154\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 3s 193ms/step - loss: 0.1497 - val_loss: 0.1317\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 3s 195ms/step - loss: 0.1095 - val_loss: 0.1246\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 3s 204ms/step - loss: 0.0893 - val_loss: 0.0966\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 3s 201ms/step - loss: 0.0810 - val_loss: 0.0845\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 3s 202ms/step - loss: 0.0778 - val_loss: 0.0860\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 3s 204ms/step - loss: 0.0711 - val_loss: 0.0924\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 3s 207ms/step - loss: 0.0709 - val_loss: 0.0828\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.0742 - val_loss: 0.0801\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 3s 207ms/step - loss: 0.0689 - val_loss: 0.0818\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.0657 - val_loss: 0.0774\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.0637 - val_loss: 0.0894\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.0632 - val_loss: 0.0711\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 3s 208ms/step - loss: 0.0614 - val_loss: 0.0745\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.0591 - val_loss: 0.0827\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.0572 - val_loss: 0.0721\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.0585 - val_loss: 0.0965\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.0620 - val_loss: 0.0705\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.0564 - val_loss: 0.0768\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.0549 - val_loss: 0.0629\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.0523 - val_loss: 0.0714\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 3s 218ms/step - loss: 0.0519 - val_loss: 0.0717\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 3s 210ms/step - loss: 0.0485 - val_loss: 0.0720\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.0512 - val_loss: 0.0663\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.0487 - val_loss: 0.0652\n",
      "Epoch 1/50\n",
      "40/40 [==============================] - 1s 15ms/step - loss: 0.0276 - val_loss: 0.0222\n",
      "Epoch 2/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0201 - val_loss: 0.0195\n",
      "Epoch 3/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0182 - val_loss: 0.0185\n",
      "Epoch 4/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0173 - val_loss: 0.0180\n",
      "Epoch 5/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0167 - val_loss: 0.0175\n",
      "Epoch 6/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0164 - val_loss: 0.0175\n",
      "Epoch 7/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0161 - val_loss: 0.0173\n",
      "Epoch 8/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0159 - val_loss: 0.0170\n",
      "Epoch 9/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0156 - val_loss: 0.0170\n",
      "Epoch 10/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0156 - val_loss: 0.0168\n",
      "Epoch 11/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0153 - val_loss: 0.0166\n",
      "Epoch 12/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0153 - val_loss: 0.0166\n",
      "Epoch 13/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.0166\n",
      "Epoch 14/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0150 - val_loss: 0.0167\n",
      "Epoch 15/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0166\n",
      "Epoch 16/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0164\n",
      "Epoch 17/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 0.0162\n",
      "Epoch 18/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 0.0161\n",
      "Epoch 19/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0146 - val_loss: 0.0165\n",
      "Epoch 20/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0145 - val_loss: 0.0163\n",
      "Epoch 21/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0145 - val_loss: 0.0161\n",
      "Epoch 22/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.0161\n",
      "Epoch 23/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0163\n",
      "Epoch 24/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0162\n",
      "Epoch 25/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0142 - val_loss: 0.0161\n",
      "Epoch 26/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0141 - val_loss: 0.0164\n",
      "Epoch 27/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0141 - val_loss: 0.0161\n",
      "Epoch 28/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0140 - val_loss: 0.0160\n",
      "Epoch 29/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0140 - val_loss: 0.0163\n",
      "Epoch 30/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0163\n",
      "Epoch 31/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0161\n",
      "Epoch 32/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0164\n",
      "Epoch 33/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0159\n",
      "Epoch 34/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0162\n",
      "Epoch 35/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0137 - val_loss: 0.0164\n",
      "Epoch 36/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0137 - val_loss: 0.0161\n",
      "Epoch 37/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0162\n",
      "Epoch 38/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0159\n",
      "Epoch 39/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0162\n",
      "Epoch 40/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0160\n",
      "Epoch 41/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0135 - val_loss: 0.0160\n",
      "Epoch 42/50\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.0135 - val_loss: 0.0158\n",
      "Epoch 43/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0161\n",
      "Epoch 44/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0161\n",
      "Epoch 45/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 0.0160\n",
      "Epoch 46/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0133 - val_loss: 0.0159\n",
      "Epoch 47/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0162\n",
      "Epoch 48/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0160\n",
      "Epoch 49/50\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.0133 - val_loss: 0.0160\n",
      "Epoch 50/50\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.0133 - val_loss: 0.0163\n",
      "Anomaly training vmin/vmax: -0.5708271 0.5441576\n",
      "Exog fitted mins: [-0.0008214734843932092, -0.006285383831709623, 0.0, -0.010621646419167519, -0.000681682548020035]\n",
      "Exog fitted maxs: [1.404210090637207, 1.0036194324493408, 1.0362836122512817, 1.0478532314300537, 3.554227113723755]\n",
      "Target land min/max (expect ~0..1): 0.0 1.0\n",
      "Target ocean unique (expect [-1]): [-1.]\n",
      "Exog land min/max (expect ~0..1): 0.0 1.0\n",
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 12, 24, 24, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_14 (ConvLSTM2D  (None, 12, 24, 24, 32)    43904     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_15 (ConvLSTM2D  (None, 24, 24, 32)        73856     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117859 (460.39 KB)\n",
      "Trainable params: 117859 (460.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 6s 214ms/step - loss: 0.5681 - val_loss: 0.2237\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 3s 194ms/step - loss: 0.1799 - val_loss: 0.1252\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 3s 195ms/step - loss: 0.1307 - val_loss: 0.1207\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 0.1122 - val_loss: 0.1198\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 0.1049 - val_loss: 0.1053\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 0.0987 - val_loss: 0.0964\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 0.0939 - val_loss: 0.0973\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 3s 207ms/step - loss: 0.0860 - val_loss: 0.0941\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 0.0842 - val_loss: 0.0954\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 3s 209ms/step - loss: 0.0921 - val_loss: 0.0941\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 0.0829 - val_loss: 0.1009\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 3s 213ms/step - loss: 0.0788 - val_loss: 0.0845\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 0.0755 - val_loss: 0.0886\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 0.0745 - val_loss: 0.0862\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 3s 231ms/step - loss: 0.0745 - val_loss: 0.0865\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 3s 211ms/step - loss: 0.0732 - val_loss: 0.0987\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 3s 207ms/step - loss: 0.0738 - val_loss: 0.0973\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.0296 - val_loss: 0.0220\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0211 - val_loss: 0.0174\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0186 - val_loss: 0.0164\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0176 - val_loss: 0.0160\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0171 - val_loss: 0.0158\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0167 - val_loss: 0.0155\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0164 - val_loss: 0.0153\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0162 - val_loss: 0.0153\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0160 - val_loss: 0.0150\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0157 - val_loss: 0.0150\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0157 - val_loss: 0.0150\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0155 - val_loss: 0.0148\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0154 - val_loss: 0.0147\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0153 - val_loss: 0.0147\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.0145\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0150 - val_loss: 0.0145\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0150 - val_loss: 0.0144\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0143\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 0.0144\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 0.0143\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 0.0143\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0146 - val_loss: 0.0145\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0145 - val_loss: 0.0143\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0145 - val_loss: 0.0142\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.0144\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0141\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0141\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0142 - val_loss: 0.0140\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0142 - val_loss: 0.0141\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0139\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0139\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0138\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0139\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0139\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0138\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0137\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.0137\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0135 - val_loss: 0.0139\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0137\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 0s 11ms/step - loss: 0.0135 - val_loss: 0.0137\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 0s 10ms/step - loss: 0.0135 - val_loss: 0.0139\n",
      "Anomaly training vmin/vmax: -0.7677059 0.77646554\n",
      "Exog fitted mins: [-0.0008214734843932092, -0.006285383831709623, 0.0, -0.13939595222473145, -0.000681682548020035]\n",
      "Exog fitted maxs: [1.404210090637207, 1.0036194324493408, 1.0362836122512817, 1.0491204261779785, 3.554227113723755]\n",
      "Target land min/max (expect ~0..1): 0.0 1.0\n",
      "Target ocean unique (expect [-1]): [-1.]\n",
      "Exog land min/max (expect ~0..1): 0.0 1.0\n",
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 12, 24, 24, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_16 (ConvLSTM2D  (None, 12, 24, 24, 32)    43904     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_17 (ConvLSTM2D  (None, 24, 24, 32)        73856     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117859 (460.39 KB)\n",
      "Trainable params: 117859 (460.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 7s 213ms/step - loss: 0.4994 - val_loss: 0.1555\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 3s 191ms/step - loss: 0.1352 - val_loss: 0.1121\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 3s 195ms/step - loss: 0.1024 - val_loss: 0.1069\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 3s 204ms/step - loss: 0.0944 - val_loss: 0.1010\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 3s 200ms/step - loss: 0.0865 - val_loss: 0.0900\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 3s 203ms/step - loss: 0.0793 - val_loss: 0.0823\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 0.0748 - val_loss: 0.0835\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 4s 207ms/step - loss: 0.0729 - val_loss: 0.0763\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 4s 208ms/step - loss: 0.0735 - val_loss: 0.0733\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 0.0699 - val_loss: 0.0765\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.0666 - val_loss: 0.0705\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 4s 211ms/step - loss: 0.0657 - val_loss: 0.0730\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.0665 - val_loss: 0.0657\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 4s 208ms/step - loss: 0.0655 - val_loss: 0.0656\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 4s 208ms/step - loss: 0.0615 - val_loss: 0.0606\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 4s 213ms/step - loss: 0.0565 - val_loss: 0.0594\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 0.0538 - val_loss: 0.0634\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.0536 - val_loss: 0.0583\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 4s 213ms/step - loss: 0.0563 - val_loss: 0.0613\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 4s 213ms/step - loss: 0.0570 - val_loss: 0.0720\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 4s 224ms/step - loss: 0.0532 - val_loss: 0.0550\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 0.0470 - val_loss: 0.0472\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 0.0436 - val_loss: 0.0466\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 4s 211ms/step - loss: 0.0480 - val_loss: 0.0530\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.0470 - val_loss: 0.0486\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 0.0433 - val_loss: 0.0434\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.0460 - val_loss: 0.0507\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 0.0457 - val_loss: 0.0541\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 4s 215ms/step - loss: 0.0424 - val_loss: 0.0425\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 0.0422 - val_loss: 0.0483\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 0.0413 - val_loss: 0.0434\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 4s 211ms/step - loss: 0.0398 - val_loss: 0.0418\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 4s 212ms/step - loss: 0.0404 - val_loss: 0.0517\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.0436 - val_loss: 0.0454\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.0434 - val_loss: 0.0539\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 4s 213ms/step - loss: 0.0443 - val_loss: 0.0633\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 4s 215ms/step - loss: 0.0438 - val_loss: 0.0431\n",
      "Epoch 1/50\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 0.0296 - val_loss: 0.0207\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.0205 - val_loss: 0.0180\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.0186 - val_loss: 0.0174\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.0178 - val_loss: 0.0170\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0173 - val_loss: 0.0167\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0169 - val_loss: 0.0166\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.0166 - val_loss: 0.0163\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0163 - val_loss: 0.0163\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0158 - val_loss: 0.0160\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0156 - val_loss: 0.0159\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0155 - val_loss: 0.0159\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0154 - val_loss: 0.0158\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.0153 - val_loss: 0.0159\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0152 - val_loss: 0.0157\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0151 - val_loss: 0.0156\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0150 - val_loss: 0.0155\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0150 - val_loss: 0.0158\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0149 - val_loss: 0.0158\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0158\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 0.0154\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.0147 - val_loss: 0.0155\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0147 - val_loss: 0.0155\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.0146 - val_loss: 0.0154\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0146 - val_loss: 0.0154\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0145 - val_loss: 0.0154\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.0155\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0144 - val_loss: 0.0154\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0144 - val_loss: 0.0155\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0143 - val_loss: 0.0153\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0142 - val_loss: 0.0155\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.0142 - val_loss: 0.0156\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0142 - val_loss: 0.0156\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0141 - val_loss: 0.0157\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0140 - val_loss: 0.0159\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0140 - val_loss: 0.0156\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0139 - val_loss: 0.0157\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0156\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0157\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0155\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0157\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0138 - val_loss: 0.0155\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 11ms/step - loss: 0.0138 - val_loss: 0.0156\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0137 - val_loss: 0.0157\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0155\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0158\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0136 - val_loss: 0.0163\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0136 - val_loss: 0.0154\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 0.0135 - val_loss: 0.0154\n",
      "Anomaly training vmin/vmax: -0.9114488 0.85479504\n",
      "Exog fitted mins: [-0.0008214715635403991, -0.006285383831709623, 0.0, -0.14141413569450378, -0.002120881574228406]\n",
      "Exog fitted maxs: [1.404210090637207, 1.0036194324493408, 1.0362836122512817, 1.0643097162246704, 3.5579004287719727]\n",
      "Target land min/max (expect ~0..1): 0.0 1.0\n",
      "Target ocean unique (expect [-1]): [-1.]\n",
      "Exog land min/max (expect ~0..1): 0.0 1.0\n",
      "Model: \"STDK_ConvLSTM_Quantiles_TF_Masked\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 12, 24, 24, 6)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d_18 (ConvLSTM2D  (None, 12, 24, 24, 32)    43904     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_lstm2d_19 (ConvLSTM2D  (None, 24, 24, 32)        73856     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 24, 24, 3)         99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117859 (460.39 KB)\n",
      "Trainable params: 117859 (460.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 7s 216ms/step - loss: 0.3628 - val_loss: 0.1320\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 4s 194ms/step - loss: 0.1046 - val_loss: 0.1068\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 4s 202ms/step - loss: 0.0811 - val_loss: 0.0815\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 4s 204ms/step - loss: 0.0707 - val_loss: 0.0783\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 4s 211ms/step - loss: 0.0685 - val_loss: 0.0778\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 4s 205ms/step - loss: 0.0644 - val_loss: 0.0726\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 4s 208ms/step - loss: 0.0625 - val_loss: 0.0721\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 4s 211ms/step - loss: 0.0654 - val_loss: 0.0758\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 4s 209ms/step - loss: 0.0622 - val_loss: 0.0714\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 4s 210ms/step - loss: 0.0589 - val_loss: 0.0693\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 4s 210ms/step - loss: 0.0553 - val_loss: 0.0629\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 4s 209ms/step - loss: 0.0560 - val_loss: 0.0700\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 4s 211ms/step - loss: 0.0582 - val_loss: 0.0721\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 4s 209ms/step - loss: 0.0564 - val_loss: 0.0566\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 4s 210ms/step - loss: 0.0515 - val_loss: 0.0673\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 4s 209ms/step - loss: 0.0556 - val_loss: 0.0586\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 4s 208ms/step - loss: 0.0495 - val_loss: 0.0526\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 4s 209ms/step - loss: 0.0515 - val_loss: 0.0555\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 4s 212ms/step - loss: 0.0468 - val_loss: 0.0500\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 4s 214ms/step - loss: 0.0495 - val_loss: 0.0622\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 4s 209ms/step - loss: 0.0498 - val_loss: 0.0535\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 4s 210ms/step - loss: 0.0443 - val_loss: 0.0499\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 4s 212ms/step - loss: 0.0440 - val_loss: 0.0483\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 4s 208ms/step - loss: 0.0437 - val_loss: 0.0484\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 4s 209ms/step - loss: 0.0416 - val_loss: 0.0455\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 4s 208ms/step - loss: 0.0426 - val_loss: 0.0496\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 4s 208ms/step - loss: 0.0409 - val_loss: 0.0404\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 4s 212ms/step - loss: 0.0392 - val_loss: 0.0419\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 4s 209ms/step - loss: 0.0362 - val_loss: 0.0437\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 4s 209ms/step - loss: 0.0395 - val_loss: 0.0430\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 4s 208ms/step - loss: 0.0395 - val_loss: 0.0431\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 4s 208ms/step - loss: 0.0368 - val_loss: 0.0452\n"
     ]
    }
   ],
   "source": [
    "for k, (tr_idx, val_test_idx) in enumerate(zip(tr_idxs, test_idxs)):\n",
    "    val_idx, tes_idx = val_test_idx[:int(len(val_test_idx)/2)], val_test_idx[int(len(val_test_idx)/2):]\n",
    "\n",
    "    fold_df_train = df_train.iloc[np.concatenate([tr_idx, val_idx])]\n",
    "    cov_das = [full_precip_da[:(val_idx[-1]+1)], full_aet_da[:(val_idx[-1]+1)], full_twsa_da[:(val_idx[-1]+1)], full_ndvi_da[:(val_idx[-1]+1)], dem_da]\n",
    "\n",
    "    fold_df_train_long = (\n",
    "        fold_df_train\n",
    "          .rename_axis(index=\"date\")\n",
    "          .reset_index()\n",
    "          .melt(id_vars=\"date\", var_name=\"station_id\", value_name=\"y\")\n",
    "    )\n",
    "    \n",
    "    fold_df_train_long[\"station_id\"] = pd.Categorical(fold_df_train_long[\"station_id\"],\n",
    "                                        categories=miss_ts.columns,\n",
    "                                        ordered=True)\n",
    "    \n",
    "    fold_df_train_long = fold_df_train_long.sort_values([\"date\", \"station_id\"], ignore_index=True)\n",
    "    \n",
    "    #### create an interpolation hold-out set\n",
    "    rng = np.random.default_rng()              # set a seed for reproducibility (optional)\n",
    "    n_cols = fold_df_train.shape[1]\n",
    "    idx = rng.choice(n_cols, size=int(0.1*n_cols), replace=False)\n",
    "    fold_val_locs = idx.tolist()\n",
    "    fold_val_locs.sort()\n",
    "    fold_train_locs = [i for i in range(fold_df_train.shape[1]) if i not in fold_val_locs]\n",
    "    fold_val_ids = fold_df_train.columns[fold_val_locs]\n",
    "    fold_train_ids = fold_df_train.columns[fold_train_locs]\n",
    "    \n",
    "    is_val = fold_df_train_long['station_id'].isin(fold_val_ids.values)\n",
    "    \n",
    "    y_train = fold_df_train_long[~is_val]\n",
    "    y_val   = fold_df_train_long[ is_val]\n",
    "    \n",
    "    coords_train = np.array([ info_file[info_file['SegmentID'] == well][['Latitude', 'Longitude']].values[0] for well in fold_train_ids ])\n",
    "    coords_val = np.array([ info_file[info_file['SegmentID'] == well][['Latitude', 'Longitude']].values[0] for well in fold_val_ids ])\n",
    "        \n",
    "    N_train = len(fold_train_ids)\n",
    "    N_val = len(fold_val_ids)\n",
    "    T = fold_df_train.shape[0]\n",
    "    \n",
    "    ## normalize coordinates to [0,1]^2 over the domain\n",
    "    lat_min, lat_max = coords_train[:, 0].min(), coords_train[:, 0].max()\n",
    "    lon_min, lon_max = coords_train[:, 1].min(), coords_train[:, 1].max()\n",
    "    \n",
    "    xy01_train = np.vstack([norm_xy(coords_train[i,0], coords_train[i,1], lat_min, lat_max, lon_min, lon_max) for i in range(N_train)])  # [N_train,2]\n",
    "    xy01_val = np.vstack([norm_xy(coords_val[i,0], coords_val[i,1], lat_min, lat_max, lon_min, lon_max) for i in range(N_val)])  # [N_val,2]\n",
    "    \n",
    "    ## normalize time stamps to [0,1] across the whole span\n",
    "    months = fold_df_train.index.values\n",
    "    t0 = months.min()\n",
    "    t1 = months.max()\n",
    "    t01_all = np.array([norm_time(ts, t0, t1) for ts in months], dtype=np.float32)  # [T]\n",
    "    \n",
    "    # build the master design (time-major stacking)\n",
    "    # coords_xy01_train: [T*N_train, 2]\n",
    "    coords_xy01_train = np.repeat(xy01_train[None, :, :], T, axis=0).reshape(T*N_train, 2)\n",
    "    # coords_xy01_val: [T*N_val, 2]\n",
    "    coords_xy01_val = np.repeat(xy01_val[None, :, :], T, axis=0).reshape(T*N_val, 2)\n",
    "    # time_scalar: [T*N_train, 1]\n",
    "    time_scalar_train = np.repeat(t01_all[:, None], N_train, axis=1).reshape(T*N_train, 1).astype(np.float32)\n",
    "    # time_scalar: [T*N_val, 1]\n",
    "    time_scalar_val = np.repeat(t01_all[:, None], N_val, axis=1).reshape(T*N_val, 1).astype(np.float32)\n",
    "    \n",
    "    ## build y in the EXACT SAME ORDER (time-major)\n",
    "    # make a dense matrix Ymat[t, i] holding y for station i at month t\n",
    "    # if y_df is long-form, pivot to [T x N] in the same order\n",
    "    month_order = list(months)  # already sorted\n",
    "    \n",
    "    # Ymat_train shape: [T, N_train] with NaN where missing\n",
    "    Ymat_train = y_train.pivot(index=\"date\", columns=\"station_id\", values=\"y\").reindex(index=month_order, columns=fold_train_ids).values\n",
    "    # Ymat_val shape: [T, N_val] with NaN where missing\n",
    "    Ymat_val = y_val.pivot(index=\"date\", columns=\"station_id\", values=\"y\").reindex(index=month_order, columns=fold_val_ids).values\n",
    "    \n",
    "    ## build a DataArray of all station points (vectorized interpolation)\n",
    "    lon_points_train = xr.DataArray(coords_train[:,1], dims=(\"station\",), coords={\"station\": fold_train_ids})\n",
    "    lat_points_train = xr.DataArray(coords_train[:,0], dims=(\"station\",), coords={\"station\": fold_train_ids})\n",
    "    lon_points_val = xr.DataArray(coords_val[:,1], dims=(\"station\",), coords={\"station\": fold_val_ids})\n",
    "    lat_points_val = xr.DataArray(coords_val[:,0], dims=(\"station\",), coords={\"station\": fold_val_ids})\n",
    "    \n",
    "    y_vec_train = Ymat_train.reshape(-1)                      # [T*N_train]\n",
    "    y_vec_val = Ymat_val.reshape(-1)                      # [T*N_train]\n",
    "    \n",
    "    keep_train = ~np.isnan(y_vec_train)\n",
    "    keep_val = ~np.isnan(y_vec_val)\n",
    "    \n",
    "    ## sample at points\n",
    "    Cs_keep_train = []\n",
    "    Cs_keep_val = []\n",
    "    pred_das_scaled = []\n",
    "    \n",
    "    for cov_da in cov_das:\n",
    "        C_train = cov_da.sel(lon=lon_points_train, lat=lat_points_train, method=\"nearest\")   # [time, station]\n",
    "        C_val = cov_da.sel(lon=lon_points_val, lat=lat_points_val, method=\"nearest\")   # [time, station]\n",
    "    \n",
    "        Cmat_train = C_train.to_numpy() # shape [T, N_train] float32, may contain NaN\n",
    "        Cmat_val = C_val.to_numpy() # shape [T, N_val] float32, may contain NaN\n",
    "    \n",
    "        if cov_da.name == 'dem':\n",
    "            Cmat_train = np.tile(Cmat_train, T)\n",
    "            Cmat_val = np.tile(Cmat_val, T)\n",
    "        \n",
    "        cov_vec_train = Cmat_train.reshape(-1, 1)                   # [T*N_train, 1]\n",
    "        cov_vec_val = Cmat_val.reshape(-1, 1)                   # [T*N_train, 1]\n",
    "    \n",
    "        C_min, C_max = cov_vec_train[keep_train].min(), cov_vec_train[keep_train].max()\n",
    "        C_keep_scaled_train = (cov_vec_train[keep_train] - C_min)/(C_max - C_min)\n",
    "        C_keep_scaled_val = (cov_vec_val[keep_val] - C_min)/(C_max - C_min)\n",
    "    \n",
    "        Cs_keep_train.append(C_keep_scaled_train)\n",
    "        Cs_keep_val.append(C_keep_scaled_val)\n",
    "        # prepare pred covariates\n",
    "        pred_das_scaled.append((cov_da - C_min) / (C_max - C_min))\n",
    "    \n",
    "    covs_train = np.concatenate(Cs_keep_train, axis=1)\n",
    "    covs_val = np.concatenate(Cs_keep_val, axis=1)\n",
    "    \n",
    "    inputs_train = {\n",
    "        \"coords_xy\":  coords_xy01_train[keep_train].astype(\"float32\"),\n",
    "        \"time_scalar\": time_scalar_train[keep_train].astype(\"float32\"),\n",
    "        \"cov_scalar_vec\": covs_train.astype(\"float32\"),   # â† scalar cov input\n",
    "    }\n",
    "    \n",
    "    inputs_val = {\n",
    "        \"coords_xy\":  coords_xy01_val[keep_val].astype(\"float32\"),\n",
    "        \"time_scalar\": time_scalar_val[keep_val].astype(\"float32\"),\n",
    "        \"cov_scalar_vec\": covs_val.astype(\"float32\"),   # â† scalar cov input\n",
    "    }\n",
    "    \n",
    "    target_train = y_vec_train[keep_train].astype(\"float32\")\n",
    "    target_val = y_vec_val[keep_val].astype(\"float32\")\n",
    "\n",
    "    model_ipol = stdk.build_interpolator_scalar_cov(\n",
    "        n_space_basis=64, n_time_basis=16,\n",
    "        ls_space=0.15, ls_time=0.10,\n",
    "        hidden=(256,128,64), dropout=0.10,\n",
    "        quantiles=(0.1,0.5,0.9), n_scalar_cov=5\n",
    "    )\n",
    "    \n",
    "    history = model_ipol.fit(inputs_train, target_train,\n",
    "                          batch_size=2048, epochs=50,\n",
    "                          validation_data=[inputs_val, target_val], verbose=1)\n",
    "    \n",
    "    T, H, W = len(months), full_precip_da.sizes[\"lat\"], full_precip_da.sizes[\"lon\"]\n",
    "    lon_grid = full_precip_da[\"lon\"].to_numpy()\n",
    "    lat_grid = full_precip_da[\"lat\"].to_numpy()\n",
    "    \n",
    "    Lon, Lat = np.meshgrid(lon_grid, lat_grid)              # [H,W]\n",
    "    X01, Y01 = norm_xy(Lat, Lon, lat_min, lat_max, lon_min, lon_max)\n",
    "    coords_xy01_grid = np.stack([X01.ravel(), Y01.ravel()], axis=1).astype(\"float32\")  # [H*W, 2]\n",
    "\n",
    "    pred_da = stdk.predict_all_months_scalar_cov(model_ipol, pred_das_scaled, months, coords_xy01_grid, t0, t1)\n",
    "\n",
    "    land_mask_da = ~cov_das[0][0].isnull()\n",
    "    \n",
    "    in_len  = 12   # use past 12 frames as context\n",
    "    out_len = 1   # 1-step ahead target for training\n",
    "    n = cov_das[0].shape[0]-in_len\n",
    "    \n",
    "    y_q50 = pred_da.sel(quantile=\"0.5\")\n",
    "    \n",
    "    da_fit = y_q50[tr_idx]  # if you really want to fit on ALL available time\n",
    "    \n",
    "    da_std = y_q50.transpose('time','lat','lon')\n",
    "    land_vals = da_fit.where(land_mask_da).values\n",
    "    vmin = np.nanmin(land_vals)\n",
    "    vmax = np.nanmax(land_vals)\n",
    "    rng  = max(vmax - vmin, 1e-8)\n",
    "    print(\"Anomaly training vmin/vmax:\", vmin, vmax)\n",
    "    \n",
    "    norm = ((da_std - vmin) / rng).clip(0, 1)\n",
    "    norm = norm.where(land_mask_da, -1.0)   # ocean sentinel\n",
    "    \n",
    "    X_targ = norm.values.astype('float32')           # (T, H, W)\n",
    "    lm_np  = land_mask_da.values                                # (H, W)\n",
    "    \n",
    "    # Optional: stack exogenous channels (must match grid/time)\n",
    "    # da_exog: (time, channel, y, x)\n",
    "    cov_stack = stack_covariates(pred_das_scaled, months)\n",
    "    ex = cov_stack.values.astype(\"float32\")  # (T,H,W,C)\n",
    "    ex_ = np.empty_like(ex)\n",
    "    \n",
    "    mins, maxs = [], []\n",
    "    for c in range(ex.shape[-1]):\n",
    "        e   = ex[..., c]                                  # (T,H,W)\n",
    "        ev  = np.where(lm_np, e, np.nan)[tr_idx]\n",
    "        cmin = float(np.nanmin(ev)); cmax = float(np.nanmax(ev)); crng = max(cmax - cmin, 1e-8)\n",
    "        mins.append(cmin); maxs.append(cmax)\n",
    "        e_norm = ((e - cmin) / crng).clip(0.0, 1.0)\n",
    "        e_norm = np.where(lm_np[None, ...], e_norm, -1.0) # ocean = -1\n",
    "        ex_[..., c] = e_norm\n",
    "    \n",
    "    print(\"Exog fitted mins:\", mins)\n",
    "    print(\"Exog fitted maxs:\", maxs)\n",
    "    \n",
    "    # Final model input (time, lat, lon, channels)\n",
    "    X_all = np.concatenate([X_targ[..., None], ex_], axis=-1)  # (T,H,W,C_total)\n",
    "    \n",
    "    T,H,W,C = X_all.shape\n",
    "    \n",
    "    # target channel\n",
    "    land_norm = X_all[..., 0][:, lm_np]  # (T, Nland)\n",
    "    print(\"Target land min/max (expect ~0..1):\", float(land_norm.min()), float(land_norm.max()))\n",
    "    ocean_norm = X_all[..., 0][:, ~lm_np]\n",
    "    print(\"Target ocean unique (expect [-1]):\", np.unique(ocean_norm))\n",
    "    \n",
    "    # exogenous channels\n",
    "    ex_land_min = min(float(X_all[..., i][:, lm_np].min()) for i in range(1,C))\n",
    "    ex_land_max = max(float(X_all[..., i][:, lm_np].max()) for i in range(1,C))\n",
    "    print(\"Exog land min/max (expect ~0..1):\", ex_land_min, ex_land_max)\n",
    "    \n",
    "    # ---- (B) sliding windows ----\n",
    "    \n",
    "    def make_windows(X, in_len, out_len):\n",
    "        T = X.shape[0]\n",
    "        X_in, Y_out = [], []\n",
    "        for t in range(T - in_len - out_len + 1):\n",
    "            X_in.append(X[t:t+in_len])                        # (in_len, H, W, C)\n",
    "            Y_out.append(X[t+in_len:t+in_len+out_len, ..., 0:1])  # target channel only -> (out_len,H,W,1)\n",
    "        X_in  = np.stack(X_in)                                # (N, in_len, H, W, C)\n",
    "        Y_out = np.stack(Y_out)                               # (N, out_len, H, W, 1)\n",
    "        return X_in, Y_out\n",
    "    \n",
    "    X_win, Y_win = make_windows(X_all, in_len, out_len)\n",
    "    \n",
    "    # For 1-step training with quantile loss, use the last target frame only: (N,H,W,1)\n",
    "    Y_last = Y_win[:, -1, ...]   # (N, H, W, 1)\n",
    "    \n",
    "    # train/val split on the **time** axis\n",
    "    X_tr, Y_tr = X_win[:-len(val_idx)], Y_last[:-len(val_idx)]\n",
    "    X_va, Y_va = X_win[-len(val_idx):], Y_last[-len(val_idx):]\n",
    "    \n",
    "    # tf.data pipelines\n",
    "    batch_size = 8\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_tr, Y_tr)).shuffle(2048).batch(batch_size).prefetch(2)\n",
    "    val_ds   = tf.data.Dataset.from_tensor_slices((X_va, Y_va)).batch(batch_size).prefetch(2)\n",
    "    \n",
    "    # Quantiles to predict\n",
    "    quantiles = [0.1, 0.5, 0.9]\n",
    "\n",
    "    scaler = {\n",
    "        \"target\": {\"vmin\": vmin, \"vmax\": vmax},\n",
    "        \"exog\":   [{\"vmin\": mn, \"vmax\": mx} for mn, mx in zip(mins, maxs)],\n",
    "    }\n",
    "    model = stdk.build_grid_forecaster((in_len, H, W, C), quantiles, lm_np, hidden=(32, 32))\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss')\n",
    "    ]\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=50, callbacks=callbacks)\n",
    "\n",
    "    steps = len(tes_idx)\n",
    "    pred_seq_norm, forecast_times = stdk.multi_step_forecast(model, X_all, steps, in_len, lm_np, quantiles, time_ref=pred_da.time.values)\n",
    "    # pred_seq shape should be (steps, H, W, Q)\n",
    "    assert pred_seq_norm.ndim == 4\n",
    "    steps_, H_, W_, Q_ = pred_seq_norm.shape\n",
    "    assert Q_ == len(quantiles)\n",
    "    \n",
    "    pred_seq_norm = np.where(land_mask_da.values[None, ..., None], pred_seq_norm, np.nan)\n",
    "    \n",
    "    rng = scaler[\"target\"][\"vmax\"] - scaler[\"target\"][\"vmin\"]\n",
    "    pred_seq_phys = pred_seq_norm * rng + scaler[\"target\"][\"vmin\"]\n",
    "    \n",
    "    pred_seq_da = xr.DataArray(\n",
    "        pred_seq_phys, dims=(\"time\", \"lat\", \"lon\", \"quantile\"),\n",
    "        coords={\"time\": forecast_times, \"lat\": pred_da.lat, \"lon\": pred_da.lon, \"quantile\": quantiles},\n",
    "        name=\"forecast\"\n",
    "    )\n",
    "    pred_da = pred_da.rio.set_spatial_dims(x_dim='lon', y_dim='lat', inplace=True)\n",
    "    pred_da = pred_da.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "    pred_da = pred_da.rio.clip(bgd_shp.geometry.values, bgd_shp.crs, all_touched=True)\n",
    "    \n",
    "    pred_seq_da = pred_seq_da.rio.set_spatial_dims(x_dim='lon', y_dim='lat', inplace=True)\n",
    "    pred_seq_da = pred_seq_da.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "    pred_seq_da = pred_seq_da.rio.clip(bgd_shp.geometry.values, bgd_shp.crs, all_touched=True)\n",
    "\n",
    "    pred_da.to_dataset(name='gwsa_pred').to_netcdf(f'{root_path}/outputs/DeepKriging/st_deepkrig_all_cov_ipol_fold_{k}.nc4')\n",
    "    pred_seq_da.to_dataset(name='gwsa_pred').to_netcdf(f'{root_path}/outputs/DeepKriging/st_deepkrig_all_cov_fcst_fold_{k}.nc4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
